{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save - MLP (AE Min 8 Node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSY\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "C:\\Users\\KSY\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"..\\\\..\\\\..\\\\data\\\\BLE_data\\\\\"\n",
    "def read_test_data(set_num, model_name, mode):\n",
    "    if model_name == 5:\n",
    "        raw_data = np.loadtxt(base_dir + \"raw_data\\\\in_\" + mode + \"_\" + str(set_num) + '.csv', delimiter=',', dtype=np.float32)\n",
    "    elif model_name == 6:\n",
    "        raw_data = np.loadtxt(base_dir + \"raw_data\\\\out_\" + mode + \"_\" + str(set_num) + '.csv', delimiter=',', dtype=np.float32)\n",
    "    b_data    = raw_data[:,0:3]\n",
    "    cell_data = raw_data[:,3:13]\n",
    "\n",
    "    return b_data, cell_data\n",
    "\n",
    "def read_denoised_test_data(set_num, model_name, mode):\n",
    "    if model_name == 5:\n",
    "        raw_data = np.loadtxt(base_dir + \"filtered_data\\\\in_\" + mode + \"_\" + str(set_num) + '_denoised.csv', delimiter=',', dtype=np.float32)\n",
    "    elif model_name == 6:\n",
    "        raw_data = np.loadtxt(base_dir + \"filtered_data\\\\out_\" + mode + \"_\" + str(set_num) + '_denoised.csv', delimiter=',', dtype=np.float32)\n",
    "    b_data    = raw_data[:,0:3]\n",
    "    cell_data = raw_data[:,3:13]\n",
    "\n",
    "    return b_data, cell_data\n",
    "\n",
    "def make_plot(y, num, begin, end):  \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    fig.patch.set_facecolor('xkcd:white')\n",
    "    x = range(len(y))\n",
    "    plt.bar(x, y, color=\"blue\")\n",
    "    plt.axis([0, num, begin, end])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name: 5 (car_in) / 6 (car_out)\n",
    "# Car-in data\n",
    "model_name = 5\n",
    "set_num = 6000\n",
    "train_set_num = int(set_num * 0.75)\n",
    "test_set_num = int(set_num * 0.25)\n",
    "\n",
    "x_in_train_noisy, y_in_train_noisy = read_test_data(train_set_num, model_name, \"train\")\n",
    "x_in_train_denoised, y_in_train_denoised = read_denoised_test_data(train_set_num, model_name, \"train\")\n",
    "x_in_test_noisy, y_in_test_noisy = read_test_data(test_set_num, model_name, \"test\")\n",
    "x_in_test_denoised, y_in_test_denoised = read_denoised_test_data(test_set_num, model_name, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name: 5 (car_in) / 6 (car_out)\n",
    "# Car-out data\n",
    "model_name = 6\n",
    "set_num = 6000\n",
    "train_set_num = int(set_num * 0.75)\n",
    "test_set_num = int(set_num * 0.25)\n",
    "\n",
    "x_out_train_noisy, y_out_train_noisy = read_test_data(train_set_num, model_name, \"train\")\n",
    "x_out_train_denoised, y_out_train_denoised = read_denoised_test_data(train_set_num, model_name, \"train\")\n",
    "x_out_test_noisy, y_out_test_noisy = read_test_data(test_set_num, model_name, \"test\")\n",
    "x_out_test_denoised, y_out_test_denoised = read_denoised_test_data(test_set_num, model_name, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car-in\n",
    "ae_x_in_train_noisy = x_in_train_noisy.astype('float32') / -100.\n",
    "ae_x_in_train_denoised = x_in_train_denoised.astype('float32') / -100.\n",
    "ae_x_in_test_noisy = x_in_test_noisy.astype('float32') / -100.\n",
    "ae_x_in_test_denoised = x_in_test_denoised.astype('float32') / -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car-out\n",
    "ae_x_out_train_noisy = x_out_train_noisy.astype('float32') / -100.\n",
    "ae_x_out_train_denoised = x_out_train_denoised.astype('float32') / -100.\n",
    "ae_x_out_test_noisy = x_out_test_noisy.astype('float32') / -100.\n",
    "ae_x_out_test_denoised = x_out_test_denoised.astype('float32') / -100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder - Denoising (Car in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "autoencoder_01 = load_model('autoencoder_01.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder - Denoising (Car out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_02 = load_model('autoencoder_02.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder - Converting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "- Autoencoder\n",
    "    - Car in (noisy)를 Autoencoder에 넣은 결과 Car in (denoised)\n",
    "    - Car in (denoised) 입력\n",
    "    - Car out (denoised) 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_03 = load_model('autoencoder_03.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder - Converting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "- Autoencoder\n",
    "    - Car in (denoised) 입력\n",
    "    - Car out (denoised) 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder_04 = load_model('autoencoder_04.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def one_hot_convert_normal(data, end_line, output_num):\n",
    "    list_data = []\n",
    "\n",
    "    for row in range(0, end_line):\n",
    "        for column in range(0, output_num):\n",
    "            if data[row][column] == 1:\n",
    "                list_data.append(column + 1)\n",
    "    return list_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "- mlp_01\n",
    "    - Car in (noisy)를 AE_03에 넣으면 결과 Car out (denoised = converted)\n",
    "    - Car out (denoised = converted)를 입력 (Input)\n",
    "- mlp_02\n",
    "    - Car in (filtered)를 AE_04에 넣어서 결과 Car out (denoised)\n",
    "    - Car out (denoised)를 입력 (Input)\n",
    "- mlp_03\n",
    "    - Car out (noisy)를 AE_02에 넣으면 Car out (denoised)\n",
    "    - Car out (denoised)를 입력 (Input)\n",
    "- mlp_04\n",
    "    - Car out (filtered)를 입력 (Input)\n",
    "- Output (정답)\n",
    "    - Car out (filtered)의 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "output_num = 10\n",
    "end_line = train_set_num * output_num\n",
    "\n",
    "oh_y_in_train_denoised = one_hot_convert_normal(y_in_train_denoised, end_line, output_num)\n",
    "oh_y_in_train_denoised = np.array(oh_y_in_train_denoised)\n",
    "\n",
    "oh_y_out_train_denoised = one_hot_convert_normal(y_out_train_denoised, end_line, output_num)\n",
    "oh_y_out_train_denoised = np.array(oh_y_out_train_denoised)\n",
    "\n",
    "# Testing data\n",
    "end_line = test_set_num * output_num\n",
    "\n",
    "oh_y_in_test_denoised = one_hot_convert_normal(y_in_test_denoised, end_line, output_num)\n",
    "oh_y_in_test_denoised = np.array(oh_y_in_test_denoised)\n",
    "\n",
    "oh_y_out_test_denoised = one_hot_convert_normal(y_out_test_denoised, end_line, output_num)\n",
    "oh_y_out_test_denoised = np.array(oh_y_out_test_denoised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.76330813\n",
      "Iteration 2, loss = 2.29766268\n",
      "Iteration 3, loss = 2.28918918\n",
      "Iteration 4, loss = 2.27855328\n",
      "Iteration 5, loss = 2.26249233\n",
      "Iteration 6, loss = 2.25240197\n",
      "Iteration 7, loss = 2.24721622\n",
      "Iteration 8, loss = 2.24198015\n",
      "Iteration 9, loss = 2.22539192\n",
      "Iteration 10, loss = 2.22396616\n",
      "Iteration 11, loss = 2.21353794\n",
      "Iteration 12, loss = 2.20112230\n",
      "Iteration 13, loss = 2.19043134\n",
      "Iteration 14, loss = 2.18029261\n",
      "Iteration 15, loss = 2.17433812\n",
      "Iteration 16, loss = 2.16908681\n",
      "Iteration 17, loss = 2.16054429\n",
      "Iteration 18, loss = 2.15954761\n",
      "Iteration 19, loss = 2.13778196\n",
      "Iteration 20, loss = 2.13703001\n",
      "Iteration 21, loss = 2.12956877\n",
      "Iteration 22, loss = 2.12476538\n",
      "Iteration 23, loss = 2.14892010\n",
      "Iteration 24, loss = 2.15277505\n",
      "Iteration 25, loss = 2.13926346\n",
      "Iteration 26, loss = 2.13612978\n",
      "Iteration 27, loss = 2.12813471\n",
      "Iteration 28, loss = 2.11620946\n",
      "Iteration 29, loss = 2.11100329\n",
      "Iteration 30, loss = 2.10247286\n",
      "Iteration 31, loss = 2.10810054\n",
      "Iteration 32, loss = 2.10269868\n",
      "Iteration 33, loss = 2.08656246\n",
      "Iteration 34, loss = 2.07866991\n",
      "Iteration 35, loss = 2.08282820\n",
      "Iteration 36, loss = 2.06541077\n",
      "Iteration 37, loss = 2.06491633\n",
      "Iteration 38, loss = 2.07257196\n",
      "Iteration 39, loss = 2.05791077\n",
      "Iteration 40, loss = 2.05413293\n",
      "Iteration 41, loss = 2.05187773\n",
      "Iteration 42, loss = 2.04079562\n",
      "Iteration 43, loss = 2.04569774\n",
      "Iteration 44, loss = 2.04186221\n",
      "Iteration 45, loss = 2.03610289\n",
      "Iteration 46, loss = 2.03769547\n",
      "Iteration 47, loss = 2.03537379\n",
      "Iteration 48, loss = 2.02856703\n",
      "Iteration 49, loss = 2.02649009\n",
      "Iteration 50, loss = 2.02417330\n",
      "Iteration 51, loss = 2.01960068\n",
      "Iteration 52, loss = 2.02976280\n",
      "Iteration 53, loss = 2.01647069\n",
      "Iteration 54, loss = 2.02043653\n",
      "Iteration 55, loss = 2.01822008\n",
      "Iteration 56, loss = 2.01325699\n",
      "Iteration 57, loss = 2.00887155\n",
      "Iteration 58, loss = 2.00751591\n",
      "Iteration 59, loss = 2.00816506\n",
      "Iteration 60, loss = 2.00828092\n",
      "Iteration 61, loss = 2.00606252\n",
      "Iteration 62, loss = 2.00330035\n",
      "Iteration 63, loss = 2.00773638\n",
      "Iteration 64, loss = 2.00350151\n",
      "Iteration 65, loss = 2.00188832\n",
      "Iteration 66, loss = 2.00394964\n",
      "Iteration 67, loss = 1.99551398\n",
      "Iteration 68, loss = 2.00122319\n",
      "Iteration 69, loss = 2.00719834\n",
      "Iteration 70, loss = 1.99379883\n",
      "Iteration 71, loss = 1.99722306\n",
      "Iteration 72, loss = 1.99643400\n",
      "Iteration 73, loss = 1.99113256\n",
      "Iteration 74, loss = 2.00037148\n",
      "Iteration 75, loss = 1.98845252\n",
      "Iteration 76, loss = 1.99421545\n",
      "Iteration 77, loss = 1.98740752\n",
      "Iteration 78, loss = 1.99599351\n",
      "Iteration 79, loss = 1.99099684\n",
      "Iteration 80, loss = 1.98649597\n",
      "Iteration 81, loss = 1.99054070\n",
      "Iteration 82, loss = 1.97690189\n",
      "Iteration 83, loss = 1.98959633\n",
      "Iteration 84, loss = 1.98281560\n",
      "Iteration 85, loss = 1.98285869\n",
      "Iteration 86, loss = 1.98869764\n",
      "Iteration 87, loss = 1.98907129\n",
      "Iteration 88, loss = 1.97771218\n",
      "Iteration 89, loss = 1.98639729\n",
      "Iteration 90, loss = 1.98508110\n",
      "Iteration 91, loss = 1.97850128\n",
      "Iteration 92, loss = 1.97750638\n",
      "Iteration 93, loss = 1.98232512\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "[MLP] Training data accuracy: 0.274\n",
      "[MLP] Testing data accuracy: 0.100\n"
     ]
    }
   ],
   "source": [
    "after_autoencoder_x_train = autoencoder_03.predict(ae_x_in_train_noisy) * (-100)\n",
    "after_autoencoder_x_test = autoencoder_02.predict(ae_x_in_test_noisy) * (-100)\n",
    "\n",
    "# MLP training\n",
    "mlp_01 = MLPClassifier(random_state=12, hidden_layer_sizes=[50, 100, 50], max_iter=100, alpha=0.001, solver='sgd', verbose=10, tol=0.000000001)\n",
    "mlp_01.fit(after_autoencoder_x_train, oh_y_out_train_denoised)\n",
    "\n",
    "# Save model\n",
    "pickle.dump(mlp_01, open('mlp_01.sav', 'wb'))\n",
    "\n",
    "# MLP accuracy\n",
    "print(\"[MLP] Training data accuracy: {:.3f}\".format(mlp_01.score(after_autoencoder_x_train, oh_y_out_train_denoised)))\n",
    "print(\"[MLP] Testing data accuracy: {:.3f}\".format(mlp_01.score(after_autoencoder_x_test, oh_y_out_test_denoised)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.87435385\n",
      "Iteration 2, loss = 0.63260047\n",
      "Iteration 3, loss = 0.43943685\n",
      "Iteration 4, loss = 0.34122509\n",
      "Iteration 5, loss = 0.28686291\n",
      "Iteration 6, loss = 0.27737577\n",
      "Iteration 7, loss = 0.20904352\n",
      "Iteration 8, loss = 0.20215978\n",
      "Iteration 9, loss = 0.19794834\n",
      "Iteration 10, loss = 0.19440180\n",
      "Iteration 11, loss = 0.33417309\n",
      "Iteration 12, loss = 0.19960321\n",
      "Iteration 13, loss = 0.18994876\n",
      "Iteration 14, loss = 0.18890954\n",
      "Iteration 15, loss = 0.18581483\n",
      "Iteration 16, loss = 0.18370765\n",
      "Iteration 17, loss = 0.18245001\n",
      "Iteration 18, loss = 0.18173389\n",
      "Iteration 19, loss = 0.18109997\n",
      "Iteration 20, loss = 0.17842423\n",
      "Iteration 21, loss = 0.17746067\n",
      "Iteration 22, loss = 0.17377552\n",
      "Iteration 23, loss = 0.17208190\n",
      "Iteration 24, loss = 0.17110021\n",
      "Iteration 25, loss = 0.17191400\n",
      "Iteration 26, loss = 0.17145865\n",
      "Iteration 27, loss = 0.17027129\n",
      "Iteration 28, loss = 0.16956473\n",
      "Iteration 29, loss = 0.16836703\n",
      "Iteration 30, loss = 0.16786849\n",
      "Iteration 31, loss = 0.16778894\n",
      "Iteration 32, loss = 0.16721763\n",
      "Iteration 33, loss = 0.16756477\n",
      "Iteration 34, loss = 0.16724748\n",
      "Iteration 35, loss = 0.16799639\n",
      "Iteration 36, loss = 0.16601668\n",
      "Iteration 37, loss = 0.16557277\n",
      "Iteration 38, loss = 0.16487332\n",
      "Iteration 39, loss = 0.16525974\n",
      "Iteration 40, loss = 0.16459786\n",
      "Iteration 41, loss = 0.16469674\n",
      "Iteration 42, loss = 0.16402696\n",
      "Iteration 43, loss = 0.16388334\n",
      "Iteration 44, loss = 0.16394390\n",
      "Iteration 45, loss = 0.16334200\n",
      "Iteration 46, loss = 0.16326712\n",
      "Iteration 47, loss = 0.16412417\n",
      "Iteration 48, loss = 0.16290351\n",
      "Iteration 49, loss = 0.16293855\n",
      "Iteration 50, loss = 0.16321544\n",
      "Iteration 51, loss = 0.16259343\n",
      "Iteration 52, loss = 0.16311640\n",
      "Iteration 53, loss = 0.16310513\n",
      "Iteration 54, loss = 0.16243411\n",
      "Iteration 55, loss = 0.16102584\n",
      "Iteration 56, loss = 0.16204782\n",
      "Iteration 57, loss = 0.16168833\n",
      "Iteration 58, loss = 0.16101397\n",
      "Iteration 59, loss = 0.16174255\n",
      "Iteration 60, loss = 0.16259775\n",
      "Iteration 61, loss = 0.16127397\n",
      "Iteration 62, loss = 0.16129709\n",
      "Iteration 63, loss = 0.16088816\n",
      "Iteration 64, loss = 0.16109237\n",
      "Iteration 65, loss = 0.16001629\n",
      "Iteration 66, loss = 0.16027236\n",
      "Iteration 67, loss = 0.16090800\n",
      "Iteration 68, loss = 0.16099025\n",
      "Iteration 69, loss = 0.16081226\n",
      "Iteration 70, loss = 0.16051992\n",
      "Iteration 71, loss = 0.15980108\n",
      "Iteration 72, loss = 0.16069103\n",
      "Iteration 73, loss = 0.15931668\n",
      "Iteration 74, loss = 0.15995186\n",
      "Iteration 75, loss = 0.16043688\n",
      "Iteration 76, loss = 0.25147402\n",
      "Iteration 77, loss = 0.17536222\n",
      "Iteration 78, loss = 0.17033115\n",
      "Iteration 79, loss = 0.16814466\n",
      "Iteration 80, loss = 0.16610013\n",
      "Iteration 81, loss = 0.16574147\n",
      "Iteration 82, loss = 0.16449865\n",
      "Iteration 83, loss = 0.16406330\n",
      "Iteration 84, loss = 0.16437450\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "[MLP] Training data accuracy: 0.949\n",
      "[MLP] Testing data accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "after_autoencoder_x_train = autoencoder_04.predict(ae_x_in_train_denoised) * (-100)\n",
    "after_autoencoder_x_test = autoencoder_04.predict(ae_x_in_test_denoised) * (-100)\n",
    "\n",
    "# MLP training\n",
    "mlp_02 = MLPClassifier(random_state=12, hidden_layer_sizes=[50, 100, 50], max_iter=100, alpha=0.001, solver='sgd', verbose=10, tol=0.000000001)\n",
    "mlp_02.fit(after_autoencoder_x_train, oh_y_out_train_denoised)\n",
    "\n",
    "# Save model\n",
    "pickle.dump(mlp_02, open('mlp_02.sav', 'wb'))\n",
    "\n",
    "# MLP accuracy\n",
    "print(\"[MLP] Training data accuracy: {:.3f}\".format(mlp_02.score(after_autoencoder_x_train, oh_y_out_train_denoised)))\n",
    "print(\"[MLP] Testing data accuracy: {:.3f}\".format(mlp_02.score(after_autoencoder_x_test, oh_y_out_test_denoised)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.93813670\n",
      "Iteration 2, loss = 0.53936913\n",
      "Iteration 3, loss = 0.32182290\n",
      "Iteration 4, loss = 0.10753402\n",
      "Iteration 5, loss = 0.08752455\n",
      "Iteration 6, loss = 0.25634815\n",
      "Iteration 7, loss = 0.08694983\n",
      "Iteration 8, loss = 0.08095346\n",
      "Iteration 9, loss = 0.07805123\n",
      "Iteration 10, loss = 0.07686258\n",
      "Iteration 11, loss = 0.20682682\n",
      "Iteration 12, loss = 0.09310731\n",
      "Iteration 13, loss = 0.08127813\n",
      "Iteration 14, loss = 0.07871038\n",
      "Iteration 15, loss = 0.07696837\n",
      "Iteration 16, loss = 0.07614683\n",
      "Iteration 17, loss = 0.07562595\n",
      "Iteration 18, loss = 0.07586452\n",
      "Iteration 19, loss = 0.07409629\n",
      "Iteration 20, loss = 0.07455581\n",
      "Iteration 21, loss = 0.07458680\n",
      "Iteration 22, loss = 0.07527694\n",
      "Iteration 23, loss = 0.07643540\n",
      "Iteration 24, loss = 0.07421620\n",
      "Iteration 25, loss = 0.07651230\n",
      "Iteration 26, loss = 0.07519313\n",
      "Iteration 27, loss = 0.07424888\n",
      "Iteration 28, loss = 0.07482788\n",
      "Iteration 29, loss = 0.15389585\n",
      "Iteration 30, loss = 0.07593400\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "[MLP] Training data accuracy: 0.981\n",
      "[MLP] Testing data accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "after_autoencoder_x_train = autoencoder_02.predict(ae_x_out_train_noisy) * (-100)\n",
    "after_autoencoder_x_test = autoencoder_02.predict(ae_x_out_test_noisy) * (-100)\n",
    "\n",
    "# MLP training\n",
    "mlp_03 = MLPClassifier(random_state=12, hidden_layer_sizes=[50, 100, 50], max_iter=100, alpha=0.001, solver='sgd', verbose=10, tol=0.000000001)\n",
    "mlp_03.fit(after_autoencoder_x_train, oh_y_out_train_denoised)\n",
    "\n",
    "# Save model\n",
    "pickle.dump(mlp_03, open('mlp_03.sav', 'wb'))\n",
    "\n",
    "# MLP accuracy\n",
    "print(\"[MLP] Training data accuracy: {:.3f}\".format(mlp_03.score(after_autoencoder_x_train, oh_y_out_train_denoised)))\n",
    "print(\"[MLP] Testing data accuracy: {:.3f}\".format(mlp_03.score(after_autoencoder_x_test, oh_y_out_test_denoised)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP_04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.99591443\n",
      "Iteration 2, loss = 0.59806178\n",
      "Iteration 3, loss = 0.45598832\n",
      "Iteration 4, loss = 0.26854034\n",
      "Iteration 5, loss = 0.27752800\n",
      "Iteration 6, loss = 0.22054149\n",
      "Iteration 7, loss = 0.15989979\n",
      "Iteration 8, loss = 0.24964898\n",
      "Iteration 9, loss = 0.14258549\n",
      "Iteration 10, loss = 0.13851355\n",
      "Iteration 11, loss = 0.13745142\n",
      "Iteration 12, loss = 0.25167155\n",
      "Iteration 13, loss = 0.12965730\n",
      "Iteration 14, loss = 0.29887488\n",
      "Iteration 15, loss = 0.16672318\n",
      "Iteration 16, loss = 0.11918356\n",
      "Iteration 17, loss = 0.11886265\n",
      "Iteration 18, loss = 0.11377988\n",
      "Iteration 19, loss = 0.11578488\n",
      "Iteration 20, loss = 0.10460775\n",
      "Iteration 21, loss = 0.10611254\n",
      "Iteration 22, loss = 0.11281847\n",
      "Iteration 23, loss = 0.10528558\n",
      "Iteration 24, loss = 0.25063383\n",
      "Iteration 25, loss = 0.13177474\n",
      "Iteration 26, loss = 0.11506028\n",
      "Iteration 27, loss = 0.11402268\n",
      "Iteration 28, loss = 0.30248045\n",
      "Iteration 29, loss = 0.12870319\n",
      "Iteration 30, loss = 0.11449467\n",
      "Iteration 31, loss = 0.10860561\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n",
      "[MLP] Training data accuracy: 0.951\n",
      "[MLP] Testing data accuracy: 0.950\n"
     ]
    }
   ],
   "source": [
    "# MLP training\n",
    "mlp_04 = MLPClassifier(random_state=12, hidden_layer_sizes=[50, 100, 50], max_iter=100, alpha=0.001, solver='sgd', verbose=10, tol=0.000000001)\n",
    "mlp_04.fit(x_out_train_denoised, oh_y_out_train_denoised)\n",
    "\n",
    "# Save model\n",
    "pickle.dump(mlp_04, open('mlp_04.sav', 'wb'))\n",
    "\n",
    "# MLP accuracy\n",
    "print(\"[MLP] Training data accuracy: {:.3f}\".format(mlp_04.score(x_out_train_denoised, oh_y_out_train_denoised)))\n",
    "print(\"[MLP] Testing data accuracy: {:.3f}\".format(mlp_04.score(x_out_test_denoised, oh_y_out_test_denoised)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
